
@misc{chung_increasing_2023,
	title = {Increasing {Diversity} {While} {Maintaining} {Accuracy}: {Text} {Data} {Generation} with {Large} {Language} {Models} and {Human} {Interventions}},
	shorttitle = {Increasing {Diversity} {While} {Maintaining} {Accuracy}},
	url = {http://arxiv.org/abs/2306.04140},
	doi = {10.48550/arXiv.2306.04140},
	abstract = {Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user's domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4\%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation.},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Chung, John Joon Young and Kamar, Ece and Amershi, Saleema},
	month = jun,
	year = {2023},
	note = {arXiv:2306.04140 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted as a long paper at ACL 2023},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/MSQX35EY/Chung et al. - 2023 - Increasing Diversity While Maintaining Accuracy T.pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/M8FQGNHK/2306.html:text/html},
}

@article{de_wynter_evaluation_2023,
	title = {An {Evaluation} on {Large} {Language} {Model} {Outputs}: {Discourse} and {Memorization}},
	issn = {29497191},
	shorttitle = {An {Evaluation} on {Large} {Language} {Model} {Outputs}},
	url = {http://arxiv.org/abs/2304.08637},
	doi = {10.1016/j.nlp.2023.100024},
	abstract = {We present an empirical evaluation of various outputs generated by nine of the most widely-available large language models (LLMs). Our analysis is done with off-the-shelf, readily-available tools. We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic. Overall, 80.0\% of the outputs evaluated contained memorized data, but outputs containing the most memorized content were also more likely to be considered of high quality. We discuss and evaluate mitigation strategies, showing that, in the models evaluated, the rate of memorized text being output is reduced. We conclude with a discussion on potential implications around what it means to learn, to memorize, and to evaluate quality text.},
	urldate = {2023-07-06},
	journal = {Natural Language Processing Journal},
	author = {de Wynter, Adrian and Wang, Xun and Sokolov, Alex and Gu, Qilong and Chen, Si-Qing},
	month = jul,
	year = {2023},
	note = {arXiv:2304.08637 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	pages = {100024},
	annote = {Comment: Preprint. Under review},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/ET5633KI/de Wynter et al. - 2023 - An Evaluation on Large Language Model Outputs Dis.pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/AIC3Z7UN/2304.html:text/html},
}

@misc{han_generating_2021,
	title = {Generating {Diverse} {Descriptions} from {Semantic} {Graphs}},
	url = {http://arxiv.org/abs/2108.05659},
	abstract = {Text generation from semantic graphs is traditionally performed with deterministic methods, which generate a unique description given an input graph. However, the generation problem admits a range of acceptable textual outputs, exhibiting lexical, syntactic and semantic variation. To address this disconnect, we present two main contributions. First, we propose a stochastic graph-to-text model, incorporating a latent variable in an encoder-decoder model, and its use in an ensemble. Second, to assess the diversity of the generated sentences, we propose a new automatic evaluation metric which jointly evaluates output diversity and quality in a multi-reference setting. We evaluate the models on WebNLG datasets in English and Russian, and show an ensemble of stochastic models produces diverse sets of generated sentences, while retaining similar quality to state-of-the-art models.},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Han, Jiuzhou and Beck, Daniel and Cohn, Trevor},
	month = aug,
	year = {2021},
	note = {arXiv:2108.05659 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: INLG 2021},
	file = {arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/TTH3595T/2108.html:text/html;Full Text PDF:/Users/charlesoneill/Zotero/storage/GBS5UFZS/Han et al. - 2021 - Generating Diverse Descriptions from Semantic Grap.pdf:application/pdf},
}

@misc{sanchez_stay_2023,
	title = {Stay on topic with {Classifier}-{Free} {Guidance}},
	url = {http://arxiv.org/abs/2306.17806},
	doi = {10.48550/arXiv.2306.17806},
	abstract = {Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q{\textbackslash}\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75{\textbackslash}\% preference for GPT4All using CFG over baseline.},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Sanchez, Guillaume and Fan, Honglu and Spangher, Alexander and Levi, Elad and Ammanamanchi, Pawan Sasanka and Biderman, Stella},
	month = jun,
	year = {2023},
	note = {arXiv:2306.17806 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/LNKVZ7EZ/Sanchez et al. - 2023 - Stay on topic with Classifier-Free Guidance.pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/36BKX9PE/2306.html:text/html},
}

@misc{sitdikov_classifiers_2022,
	title = {Classifiers are {Better} {Experts} for {Controllable} {Text} {Generation}},
	url = {http://arxiv.org/abs/2205.07276},
	doi = {10.48550/arXiv.2205.07276},
	abstract = {This paper proposes a simple method for controllable text generation based on weighting logits with a free-form classifier, namely CAIF sampling. Using an arbitrary text classifier, we adjust a small part of a language model's logits and guide text generation towards or away from classifier prediction. We experimented with toxicity avoidance and sentiment control tasks and showed that the proposed method significantly outperforms recent PPLM, GeDi, and DExperts on PPL and task accuracy metrics based on the external classifier of generated texts. In addition, compared to other approaches, it is easier to implement and tune and has significantly fewer restrictions and requirements.},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Sitdikov, Askhat and Balagansky, Nikita and Gavrilov, Daniil and Markov, Alexander},
	month = nov,
	year = {2022},
	note = {arXiv:2205.07276 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/RZXVJG9I/Sitdikov et al. - 2022 - Classifiers are Better Experts for Controllable Te.pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/TNQIA9TM/2205.html:text/html},
}

@misc{li_contrastive_2022,
	title = {Contrastive {Decoding}: {Open}-ended {Text} {Generation} as {Optimization}},
	shorttitle = {Contrastive {Decoding}},
	url = {http://arxiv.org/abs/2210.15097},
	doi = {10.48550/arXiv.2210.15097},
	abstract = {Likelihood, although useful as a training loss, is a poor search objective for guiding open-ended generation from language models (LMs). Existing generation algorithms must avoid both unlikely strings, which are incoherent, and highly likely ones, which are short and repetitive. We propose contrastive decoding (CD), a more reliable search objective that returns the difference between likelihood under a large LM (called the expert, e.g. OPT-13b) and a small LM (called the amateur, e.g. OPT-125m). CD is inspired by the fact that the failures of larger LMs are even more prevalent in smaller LMs, and that this difference signals exactly which texts should be preferred. CD requires zero training, and produces higher quality text than decoding from the larger LM alone. It also generalizes across model types (OPT and GPT2) and significantly outperforms four strong decoding algorithms in automatic and human evaluations.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori and Zettlemoyer, Luke and Lewis, Mike},
	month = oct,
	year = {2022},
	note = {arXiv:2210.15097 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/QTWP58PI/Li et al. - 2022 - Contrastive Decoding Open-ended Text Generation a.pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/KQRETJG9/2210.html:text/html},
}

@misc{shi_trusting_2023,
	title = {Trusting {Your} {Evidence}: {Hallucinate} {Less} with {Context}-aware {Decoding}},
	shorttitle = {Trusting {Your} {Evidence}},
	url = {http://arxiv.org/abs/2305.14739},
	doi = {10.48550/arXiv.2305.14739},
	abstract = {Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3\% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Shi, Weijia and Han, Xiaochuang and Lewis, Mike and Tsvetkov, Yulia and Zettlemoyer, Luke and Yih, Scott Wen-tau},
	month = may,
	year = {2023},
	note = {arXiv:2305.14739 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/UXSA8SF4/Shi et al. - 2023 - Trusting Your Evidence Hallucinate Less with Cont.pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/9LF4LVAZ/2305.html:text/html},
}

@misc{ho_classifier-free_2022,
	title = {Classifier-{Free} {Diffusion} {Guidance}},
	url = {http://arxiv.org/abs/2207.12598},
	abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Ho, Jonathan and Salimans, Tim},
	month = jul,
	year = {2022},
	note = {arXiv:2207.12598 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: A short version of this paper appeared in the NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications: https://openreview.net/pdf?id=qw8AKxfYbI},
	file = {arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/EDBM4NGV/2207.html:text/html;Full Text:/Users/charlesoneill/Zotero/storage/WVTMGP7Q/Ho and Salimans - 2022 - Classifier-Free Diffusion Guidance.pdf:application/pdf;Full Text PDF:/Users/charlesoneill/Zotero/storage/LCQH8W8A/Ho and Salimans - 2022 - Classifier-Free Diffusion Guidance.pdf:application/pdf},
}

@misc{platzer_rule-adhering_2022,
	title = {Rule-adhering synthetic data -- the lingua franca of learning},
	url = {http://arxiv.org/abs/2209.06679},
	abstract = {AI-generated synthetic data allows to distill the general patterns of existing data, that can then be shared safely as granular-level representative, yet novel data samples within the original semantics. In this work we explore approaches of incorporating domain expertise into the data synthesis, to have the statistical properties as well as pre-existing domain knowledge of rules be represented. The resulting synthetic data generator, that can be probed for any number of new samples, can then serve as a common source of intelligence, as a lingua franca of learning, consumable by humans and machines alike. We demonstrate the concept for a publicly available data set, and evaluate its benefits via descriptive analysis as well as a downstream ML model.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Platzer, Michael and Krchova, Ivona},
	month = sep,
	year = {2022},
	note = {arXiv:2209.06679 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 3 pages, 6 figures, accepted at Small Data workshop at 3rd ACM International Conference on AI in Finance (2022)},
	file = {arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/M2YZXP4I/2209.html:text/html;Full Text PDF:/Users/charlesoneill/Zotero/storage/63B2IQJ5/Platzer and Krchova - 2022 - Rule-adhering synthetic data -- the lingua franca .pdf:application/pdf},
}

@misc{veselovsky_generating_2023,
	title = {Generating {Faithful} {Synthetic} {Data} with {Large} {Language} {Models}: {A} {Case} {Study} in {Computational} {Social} {Science}},
	shorttitle = {Generating {Faithful} {Synthetic} {Data} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.15041},
	doi = {10.48550/arXiv.2305.15041},
	abstract = {Large Language Models (LLMs) have democratized synthetic data generation, which in turn has the potential to simplify and broaden a wide gamut of NLP tasks. Here, we tackle a pervasive problem in synthetic data generation: its generative distribution often differs from the distribution of real-world data researchers care about (in other words, it is unfaithful). In a case study on sarcasm detection, we study three strategies to increase the faithfulness of synthetic data: grounding, filtering, and taxonomy-based generation. We evaluate these strategies using the performance of classifiers trained with generated synthetic data on real-world data. While all three strategies improve the performance of classifiers, we find that grounding works best for the task at hand. As synthetic data generation plays an ever-increasing role in NLP research, we expect this work to be a stepping stone in improving its utility. We conclude this paper with some recommendations on how to generate high(er)-fidelity synthetic data for specific tasks.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Veselovsky, Veniamin and Ribeiro, Manoel Horta and Arora, Akhil and Josifoski, Martin and Anderson, Ashton and West, Robert},
	month = may,
	year = {2023},
	note = {arXiv:2305.15041 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 8 pages},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/9MM6S9EY/Veselovsky et al. - 2023 - Generating Faithful Synthetic Data with Large Lang.pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/HJ5GMSVP/2305.html:text/html},
}

@misc{he_generate_2022,
	title = {Generate, {Annotate}, and {Learn}: {NLP} with {Synthetic} {Text}},
	shorttitle = {Generate, {Annotate}, and {Learn}},
	url = {http://arxiv.org/abs/2106.06168},
	doi = {10.48550/arXiv.2106.06168},
	abstract = {This paper studies the use of language models as a source of synthetic unlabeled text for NLP. We formulate a general framework called ``generate, annotate, and learn (GAL)'' to take advantage of synthetic text within knowledge distillation, self-training, and few-shot learning applications. To generate high-quality task-specific text, we either fine-tune LMs on inputs from the task of interest, or prompt large LMs with few examples. We use the best available classifier to annotate synthetic text with soft pseudo labels for knowledge distillation and self-training, and use LMs to obtain hard labels for few-shot learning. We train new supervised models on the combination of labeled and pseudo-labeled data, which results in significant gains across several applications. We investigate key components of GAL and present theoretical and empirical arguments against the use of class-conditional LMs to generate synthetic labeled text instead of unlabeled text. GAL achieves new state-of-the-art knowledge distillation results for 6-layer transformers on the GLUE leaderboard.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {He, Xuanli and Nassar, Islam and Kiros, Jamie and Haffari, Gholamreza and Norouzi, Mohammad},
	month = may,
	year = {2022},
	note = {arXiv:2106.06168 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: accepted to TACL2022},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/HECHJNK2/He et al. - 2022 - Generate, Annotate, and Learn NLP with Synthetic .pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/E9ARGIRV/2106.html:text/html;Full Text:/Users/charlesoneill/Zotero/storage/XKDEGEI6/He et al. - 2022 - Generate, Annotate, and Learn NLP with Synthetic .pdf:application/pdf},
}

@misc{josifoski_exploiting_2023,
	title = {Exploiting {Asymmetry} for {Synthetic} {Training} {Data} {Generation}: {SynthIE} and the {Case} of {Information} {Extraction}},
	shorttitle = {Exploiting {Asymmetry} for {Synthetic} {Training} {Data} {Generation}},
	url = {http://arxiv.org/abs/2303.04132},
	abstract = {Large language models (LLMs) show great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by the LLM: we show that, for problems with structured outputs, it is possible to prompt an LLM to perform the task in the opposite direction, to generate plausible text for the target structure. Leveraging the asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, demonstrate its superior quality compared to existing datasets in a human evaluation and use it to finetune small models (220M and 770M parameters). The models we introduce, SynthIE, outperform existing baselines of comparable size with a substantial gap of 57 and 79 absolute points in micro and macro F1, respectively. Code, data, and models are available at https://github.com/epfl-dlab/SynthIE.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Josifoski, Martin and Sakota, Marija and Peyrard, Maxime and West, Robert},
	month = mar,
	year = {2023},
	note = {arXiv:2303.04132 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/F6QX4BL9/2303.html:text/html;Full Text:/Users/charlesoneill/Zotero/storage/7YFLETB8/Josifoski et al. - 2023 - Exploiting Asymmetry for Synthetic Training Data G.pdf:application/pdf;Full Text PDF:/Users/charlesoneill/Zotero/storage/GL9I992A/Josifoski et al. - 2023 - Exploiting Asymmetry for Synthetic Training Data G.pdf:application/pdf},
}

@misc{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2105.05233},
	doi = {10.48550/arXiv.2105.05233},
	abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\${\textbackslash}times\$128, 4.59 on ImageNet 256\${\textbackslash}times\$256, and 7.72 on ImageNet 512\${\textbackslash}times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\${\textbackslash}times\$256 and 3.85 on ImageNet 512\${\textbackslash}times\$512. We release our code at https://github.com/openai/guided-diffusion},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	month = jun,
	year = {2021},
	note = {arXiv:2105.05233 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Added compute requirements, ImageNet 256\${\textbackslash}times\$256 upsampling FID and samples, DDIM guided sampler, fixed typos},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/2ER2YUBP/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/HXREI9AL/2105.html:text/html},
}

@misc{alaa_how_2022,
	title = {How {Faithful} is your {Synthetic} {Data}? {Sample}-level {Metrics} for {Evaluating} and {Auditing} {Generative} {Models}},
	shorttitle = {How {Faithful} is your {Synthetic} {Data}?},
	url = {http://arxiv.org/abs/2102.08921},
	doi = {10.48550/arXiv.2102.08921},
	abstract = {Devising domain- and model-agnostic evaluation metrics for generative models is an important and as yet unresolved problem. Most existing metrics, which were tailored solely to the image synthesis setup, exhibit a limited capacity for diagnosing the different modes of failure of generative models across broader application domains. In this paper, we introduce a 3-dimensional evaluation metric, (\${\textbackslash}alpha\$-Precision, \${\textbackslash}beta\$-Recall, Authenticity), that characterizes the fidelity, diversity and generalization performance of any generative model in a domain-agnostic fashion. Our metric unifies statistical divergence measures with precision-recall analysis, enabling sample- and distribution-level diagnoses of model fidelity and diversity. We introduce generalization as an additional, independent dimension (to the fidelity-diversity trade-off) that quantifies the extent to which a model copies training data -- a crucial performance indicator when modeling sensitive data with requirements on privacy. The three metric components correspond to (interpretable) probabilistic quantities, and are estimated via sample-level binary classification. The sample-level nature of our metric inspires a novel use case which we call model auditing, wherein we judge the quality of individual samples generated by a (black-box) model, discarding low-quality samples and hence improving the overall model performance in a post-hoc manner.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Alaa, Ahmed M. and van Breugel, Boris and Saveliev, Evgeny and van der Schaar, Mihaela},
	month = jul,
	year = {2022},
	note = {arXiv:2102.08921 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/BJ23D84I/Alaa et al. - 2022 - How Faithful is your Synthetic Data Sample-level .pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/BTMUHZ2L/2102.html:text/html;Full Text:/Users/charlesoneill/Zotero/storage/5DNCCMF8/Alaa et al. - 2022 - How Faithful is your Synthetic Data Sample-level .pdf:application/pdf},
}

@misc{brock_large_2019,
	title = {Large {Scale} {GAN} {Training} for {High} {Fidelity} {Natural} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/1809.11096},
	doi = {10.48550/arXiv.1809.11096},
	abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	month = feb,
	year = {2019},
	note = {arXiv:1809.11096 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/UH5QHS6Q/Brock et al. - 2019 - Large Scale GAN Training for High Fidelity Natural.pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/EELCVCK4/1809.html:text/html},
}

@misc{moller_is_2023,
	title = {Is a prompt and a few samples all you need? {Using} {GPT}-4 for data augmentation in low-resource classification tasks},
	shorttitle = {Is a prompt and a few samples all you need?},
	url = {http://arxiv.org/abs/2304.13861},
	doi = {10.48550/arXiv.2304.13861},
	abstract = {Obtaining and annotating data can be expensive and time-consuming, especially in complex, low-resource domains. We use GPT-4 and ChatGPT to augment small labeled datasets with synthetic data via simple prompts, in three different classification tasks with varying complexity. For each task, we randomly select a base sample of 500 texts to generate 5,000 new synthetic samples. We explore two augmentation strategies: one that preserves original label distribution and another that balances the distribution. Using a progressively larger training sample size, we train and evaluate a 110M parameter multilingual language model on the real and synthetic data separately. We also test GPT-4 and ChatGPT in a zero-shot setting on the test sets. We observe that GPT-4 and ChatGPT have strong zero-shot performance across all tasks. We find that data augmented with synthetic samples yields a good downstream performance, and particularly aids in low-resource settings, such as in identifying rare classes. Human-annotated data exhibits a strong predictive power, overtaking synthetic data in two out of the three tasks. This finding highlights the need for more complex prompts for synthetic datasets to consistently surpass human-generated ones.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Møller, Anders Giovanni and Dalsgaard, Jacob Aarup and Pera, Arianna and Aiello, Luca Maria},
	month = apr,
	year = {2023},
	note = {arXiv:2304.13861 [physics]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Physics - Physics and Society},
	annote = {Comment: 12 pages, 4 figures, 4 tables},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/D2JVR8CQ/Møller et al. - 2023 - Is a prompt and a few samples all you need Using .pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/PWRXJKQW/2304.html:text/html},
}

@misc{eldan_tinystories_2023,
	title = {{TinyStories}: {How} {Small} {Can} {Language} {Models} {Be} and {Still} {Speak} {Coherent} {English}?},
	shorttitle = {{TinyStories}},
	url = {http://arxiv.org/abs/2305.07759},
	doi = {10.48550/arXiv.2305.07759},
	abstract = {Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Eldan, Ronen and Li, Yuanzhi},
	month = may,
	year = {2023},
	note = {arXiv:2305.07759 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/IXB6WLHB/Eldan and Li - 2023 - TinyStories How Small Can Language Models Be and .pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/ZLESX9GW/2305.html:text/html},
}

@misc{feng_survey_2021,
	title = {A {Survey} of {Data} {Augmentation} {Approaches} for {NLP}},
	url = {http://arxiv.org/abs/2105.03075},
	doi = {10.48550/arXiv.2105.03075},
	abstract = {Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area. We also present a GitHub repository with a paper list that will be continuously updated at https://github.com/styfeng/DataAug4NLP},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Feng, Steven Y. and Gangal, Varun and Wei, Jason and Chandar, Sarath and Vosoughi, Soroush and Mitamura, Teruko and Hovy, Eduard},
	month = dec,
	year = {2021},
	note = {arXiv:2105.03075 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted to ACL 2021 Findings. GitHub repo with paper list at https://github.com/styfeng/DataAug4NLP ; Talk at https://www.youtube.com/watch?v=kNBVesKUZCk\&ab\_channel=StevenFeng ; Podcast at https://www.youtube.com/watch?v=qmqyT\_97Poc\&ab\_channel=GradientFlow and https://thedataexchange.media/data-augmentation-in-natural-language-processing},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/BPM8HHBV/Feng et al. - 2021 - A Survey of Data Augmentation Approaches for NLP.pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/M8P63V5U/2105.html:text/html},
}

@misc{hernandez_scaling_2021,
	title = {Scaling {Laws} for {Transfer}},
	url = {http://arxiv.org/abs/2102.01293},
	doi = {10.48550/arXiv.2102.01293},
	abstract = {We study empirical scaling laws for transfer learning between distributions in an unsupervised, fine-tuning setting. When we train increasingly large neural networks from-scratch on a fixed-size dataset, they eventually become data-limited and stop improving in performance (cross-entropy loss). When we do the same for models pre-trained on a large language dataset, the slope in performance gains is merely reduced rather than going to zero. We calculate the effective data "transferred" from pre-training by determining how much data a transformer of the same size would have required to achieve the same loss when training from scratch. In other words, we focus on units of data while holding everything else fixed. We find that the effective data transferred is described well in the low data regime by a power-law of parameter count and fine-tuning dataset size. We believe the exponents in these power-laws correspond to measures of the generality of a model and proximity of distributions (in a directed rather than symmetric sense). We find that pre-training effectively multiplies the fine-tuning dataset size. Transfer, like overall performance, scales predictably in terms of parameters, data, and compute.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Hernandez, Danny and Kaplan, Jared and Henighan, Tom and McCandlish, Sam},
	month = feb,
	year = {2021},
	note = {arXiv:2102.01293 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 19 pages, 15 figures},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/VD5HB33E/Hernandez et al. - 2021 - Scaling Laws for Transfer.pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/L2NV83RZ/2102.html:text/html},
}

@inproceedings{yang_generative_2020,
	title = {Generative {Data} {Augmentation} for {Commonsense} {Reasoning}},
	url = {http://arxiv.org/abs/2004.11546},
	doi = {10.18653/v1/2020.findings-emnlp.90},
	abstract = {Recent advances in commonsense reasoning depend on large-scale human-annotated training data to achieve peak performance. However, manual curation of training examples is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit on. We investigate G-DAUG{\textasciicircum}C, a novel generative data augmentation method that aims to achieve more accurate and robust learning in the low-resource setting. Our approach generates synthetic examples using pretrained language models, and selects the most informative and diverse set of examples for data augmentation. In experiments with multiple commonsense reasoning benchmarks, G-DAUG{\textasciicircum}C consistently outperforms existing data augmentation methods based on back-translation, and establishes a new state-of-the-art on WinoGrande, CODAH, and CommonsenseQA. Further, in addition to improvements in in-distribution accuracy, G-DAUG{\textasciicircum}C-augmented training also enhances out-of-distribution generalization, showing greater robustness against adversarial or perturbed examples. Our analysis demonstrates that G-DAUG{\textasciicircum}C produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance. Our findings encourage future research toward generative data augmentation to enhance both in-distribution learning and out-of-distribution generalization.},
	urldate = {2023-07-10},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	author = {Yang, Yiben and Malaviya, Chaitanya and Fernandez, Jared and Swayamdipta, Swabha and Bras, Ronan Le and Wang, Ji-Ping and Bhagavatula, Chandra and Choi, Yejin and Downey, Doug},
	year = {2020},
	note = {arXiv:2004.11546 [cs]},
	keywords = {Computer Science - Computation and Language},
	pages = {1008--1025},
	annote = {Comment: Findings of the Association for Computational Linguistics: EMNLP 2020},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/WHQTIMFC/Yang et al. - 2020 - Generative Data Augmentation for Commonsense Reaso.pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/ZKCEF9RU/2004.html:text/html},
}

@misc{kumar_data_2021,
	title = {Data {Augmentation} using {Pre}-trained {Transformer} {Models}},
	url = {http://arxiv.org/abs/2003.02245},
	doi = {10.48550/arXiv.2003.02245},
	abstract = {Language model based pre-trained models such as BERT have provided significant gains across different NLP tasks. In this paper, we study different types of transformer based pre-trained models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. Additionally, on three classification benchmarks, pre-trained Seq2Seq model outperforms other data augmentation methods in a low-resource setting. Further, we explore how different pre-trained model based data augmentation differs in-terms of data diversity, and how well such methods preserve the class-label information.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Kumar, Varun and Choudhary, Ashutosh and Cho, Eunah},
	month = jan,
	year = {2021},
	note = {arXiv:2003.02245 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: In Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems @ AACL 2020; Code: https://github.com/varinf/TransformersDataAugmentation},
	file = {arXiv Fulltext PDF:/Users/charlesoneill/Zotero/storage/ZEYAM53D/Kumar et al. - 2021 - Data Augmentation using Pre-trained Transformer Mo.pdf:application/pdf;arXiv.org Snapshot:/Users/charlesoneill/Zotero/storage/MW7FQMR9/2003.html:text/html},
}
