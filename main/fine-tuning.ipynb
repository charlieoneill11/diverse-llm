{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
    "!pip install -q datasets bitsandbytes einops wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/charlesoneill/.cache/huggingface/datasets/json/default-08a626f129b64a6f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"../data/abstracts.json\", split=\"train\")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    output_texts = []\n",
    "    for i in range(len(examples['prompt'])):\n",
    "        text = f\"### Instruction: {examples['prompt'][i]}\\n ### Hypothesis: {examples['completion'][i]}\"\n",
    "        output_texts.append({'text': text})\n",
    "    return output_texts\n",
    "\n",
    "output_texts = formatting_prompts_func(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '### Instruction: Generate a scientific hypothesis about astronomy in the style of an Arxiv paper.\\n ### Hypothesis: Chemical Cartography, or mapping, of our Galaxy has the potential to fully\\\\ntransform our view of its structure and formation. In this work, we use\\\\nchemical cartography to explore the metallicity distribution of OBAF-type disk\\\\nstars from the LAMOST survey and a complementary sample of disk giant stars\\\\nfrom Gaia DR3. We use these samples to constrain the radial and vertical\\\\nmetallicity gradients across the Galactic disk. We also explore whether there\\\\nare detectable azimuthal variations in the metallicity distribution on top of\\\\nthe radial gradient. For the OBAF-type star sample from LAMOST, we find a\\\\nradial metallicity gradient of $\\\\Delta$[Fe/H]/$\\\\Delta$R $\\\\sim -0.078 \\\\pm 0.001$\\\\ndex/kpc in the plane of the disk and a vertical metallicity gradient of\\\\n$\\\\Delta$[Fe/H]/$\\\\Delta$Z $\\\\sim -0.15 \\\\pm 0.01$ dex/kpc in the solar\\\\nneighborhood. The radial gradient becomes shallower with increasing vertical\\\\nheight while the vertical gradient becomes shallower with increasing\\\\nGalactocentric radius, consistent with other studies. We also find detectable\\\\nspatially-dependent azimuthal variations on top of the radial metallicity\\\\ngradient at the level of $\\\\sim$0.10 dex. Interestingly, the azimuthal\\\\nvariations appear be close to the Galactic spiral arms in one dataset (Gaia\\\\nDR3) but not the other (LAMOST). These results suggest that there is azimuthal\\\\nstructure in the Galactic metallicity distribution and that in some cases it is\\\\nco-located with spiral arms.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction: Generate a scientific hypothesis about astronomy in the style of an Arxiv paper.\\n ### Hypothesis: Chemical Cartography, or mapping, of our Galaxy has the potential to fully\\\\ntransform our view of its structure and formation. In this work, we use\\\\nchemical cartography to explore the metallicity distribution of OBAF-type disk\\\\nstars from the LAMOST survey and a complementary sample of disk giant stars\\\\nfrom Gaia DR3. We use these samples to constrain the radial and vertical\\\\nmetallicity gradients across the Galactic disk. We also explore whether there\\\\nare detectable azimuthal variations in the metallicity distribution on top of\\\\nthe radial gradient. For the OBAF-type star sample from LAMOST, we find a\\\\nradial metallicity gradient of $\\\\Delta$[Fe/H]/$\\\\Delta$R $\\\\sim -0.078 \\\\pm 0.001$\\\\ndex/kpc in the plane of the disk and a vertical metallicity gradient of\\\\n$\\\\Delta$[Fe/H]/$\\\\Delta$Z $\\\\sim -0.15 \\\\pm 0.01$ dex/kpc in the solar\\\\nneighborhood. The radial gradient becomes shallower with increasing vertical\\\\nheight while the vertical gradient becomes shallower with increasing\\\\nGalactocentric radius, consistent with other studies. We also find detectable\\\\nspatially-dependent azimuthal variations on top of the radial metallicity\\\\ngradient at the level of $\\\\sim$0.10 dex. Interestingly, the azimuthal\\\\nvariations appear be close to the Galactic spiral arms in one dataset (Gaia\\\\nDR3) but not the other (LAMOST). These results suggest that there is azimuthal\\\\nstructure in the Galactic metallicity distribution and that in some cases it is\\\\nco-located with spiral arms.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"query_key_value\",\n",
    "        \"dense\",\n",
    "        \"dense_h_to_4h\",\n",
    "        \"dense_4h_to_h\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"./results\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 10\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 500\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 512\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    formatting_func=formatting_prompts_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diverse-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
