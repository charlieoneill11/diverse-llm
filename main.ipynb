{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad1403a-7824-4294-8c5c-ff172f4b9b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers einops accelerate xformers\n",
    "# !pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d24615-d932-4ab1-9733-5fc53fe9d39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b731cb-d3a4-4406-b7e5-561170bfbd20",
   "metadata": {},
   "source": [
    "## Understanding logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e663857f-9929-4d93-bef5-1ce58a1740dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "logits = outputs.logits\n",
    "print(logits)\n",
    "\n",
    "# convert these logits to probabilities\n",
    "probs = F.softmax(logits, dim=1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4192297-d44e-4958-8ef0-44fd907dd2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c682ed1b7eb5480fa3980d77dc7f4e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e30c18e-5550-4783-8539-e11c4f8b82b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Write a poem about Valencia.\n",
      "Valencia, city of sun and sky,\n",
      "The city that's made of pure white,\n",
      "Where fountains of crystal clear light,\n",
      "Are mirrored in the night.\n",
      "\n",
      "Valencia, city of the arts,\n",
      "The city where you find the heart,\n",
      "Where Picasso once his brush did paint,\n",
      "A masterpiece, on its walls.\n",
      "\n",
      "Valencia, city of the sea,\n",
      "The city that has its own beat,\n",
      "Where sailboats dance in perfect time,\n",
      "As the sun sets without a sound.\n",
      "\n",
      "Valencia, city of a thousand faces,\n",
      "Each one a story to tell,\n",
      "Where the past intertwines with the present,\n",
      "And where beauty is forever in every detail.\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "   \"Write a poem about Valencia.\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "359b4105-79a4-4f80-8e12-7f4e84b45fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4300fdbd13d6441ab1e638117dcc6a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\", torch_dtype=torch.bfloat16, \n",
    "                                             trust_remote_code=True, device_map=\"auto\",)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ba63d87-3a2c-464d-af08-88614c50d105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[9856,   23,  491, 3696,  304, 7209]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Got unexpected arguments: {'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]])}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-3ed2b57d4908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Batch size 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/2f5c3cd4eace6be6c0f12981f377fb35e5bf6ee5/modelling_RW.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    747\u001b[0m             )\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeprecated_arguments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got unexpected arguments: {deprecated_arguments}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Got unexpected arguments: {'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]])}"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").to(device)\n",
    "print(inputs)\n",
    "labels = torch.tensor([1]).unsqueeze(0).to(device)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ef8d8aa-a1b7-4e4c-9225-284961eecabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer(\" \", return_tensors='pt')\n",
    "device = 'cuda:0'\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=prompt['input_ids'].to(device),\n",
    "    attention_mask=prompt['attention_mask'].to(device),\n",
    "    max_new_tokens=125,\n",
    "    do_sample=True,\n",
    "    output_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49b6a67e-6bc8-49a7-a68c-c136d72e256f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgeneration_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerationConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlogits_processor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogitsProcessorList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstopping_criteria\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopping_criteria\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStoppingCriteriaList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprefix_allowed_tokens_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msynced_gpus\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0massistant_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PreTrainedModel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstreamer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BaseStreamer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGreedySearchEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGreedySearchDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampleEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampleDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeamSearchEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeamSearchDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeamSampleEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeamSampleDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContrastiveSearchEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContrastiveSearchDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Generates sequences of token ids for models with a language modeling head.\n",
       "\n",
       "<Tip warning={true}>\n",
       "\n",
       "Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
       "model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
       "parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
       "\n",
       "For an overview of generation strategies and code examples, check out the [following\n",
       "guide](../generation_strategies).\n",
       "\n",
       "</Tip>\n",
       "\n",
       "Parameters:\n",
       "    inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
       "        The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
       "        method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
       "        should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
       "        `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
       "    generation_config (`~generation.GenerationConfig`, *optional*):\n",
       "        The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
       "        passed to generate matching the attributes of `generation_config` will override them. If\n",
       "        `generation_config` is not provided, the default will be used, which had the following loading\n",
       "        priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
       "        configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
       "        default values, whose documentation should be checked to parameterize generation.\n",
       "    logits_processor (`LogitsProcessorList`, *optional*):\n",
       "        Custom logits processors that complement the default logits processors built from arguments and\n",
       "        generation config. If a logit processor is passed that is already created with the arguments or a\n",
       "        generation config an error is thrown. This feature is intended for advanced users.\n",
       "    stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
       "        Custom stopping criteria that complement the default stopping criteria built from arguments and a\n",
       "        generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
       "        generation config an error is thrown. This feature is intended for advanced users.\n",
       "    prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
       "        If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
       "        provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
       "        `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
       "        on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
       "        for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
       "        Retrieval](https://arxiv.org/abs/2010.00904).\n",
       "    synced_gpus (`bool`, *optional*):\n",
       "        Whether to continue running the while loop until max_length. Unless overridden this flag will be set to\n",
       "        `True` under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished\n",
       "        generating before other GPUs. Otherwise it'll be set to `False`.\n",
       "    assistant_model (`PreTrainedModel`, *optional*):\n",
       "        An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
       "        same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model\n",
       "        is much faster than running generation with the model you're calling generate from. As such, the\n",
       "        assistant model should be much smaller.\n",
       "    streamer (`BaseStreamer`, *optional*):\n",
       "        Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
       "        through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
       "    kwargs:\n",
       "        Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n",
       "        forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
       "        specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
       "\n",
       "Return:\n",
       "    [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
       "    or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
       "\n",
       "        If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
       "        [`~utils.ModelOutput`] types are:\n",
       "\n",
       "            - [`~generation.GreedySearchDecoderOnlyOutput`],\n",
       "            - [`~generation.SampleDecoderOnlyOutput`],\n",
       "            - [`~generation.BeamSearchDecoderOnlyOutput`],\n",
       "            - [`~generation.BeamSampleDecoderOnlyOutput`]\n",
       "\n",
       "        If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
       "        [`~utils.ModelOutput`] types are:\n",
       "\n",
       "            - [`~generation.GreedySearchEncoderDecoderOutput`],\n",
       "            - [`~generation.SampleEncoderDecoderOutput`],\n",
       "            - [`~generation.BeamSearchEncoderDecoderOutput`],\n",
       "            - [`~generation.BeamSampleEncoderDecoderOutput`]\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.8/site-packages/transformers/generation/utils.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?model.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbdcf2d8-a5e0-475a-938e-cca3ef172765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  209,   187,   395,   253,   187, 18941,   187,   249,   253,   187,\n",
      "          5045,   187,  1171,   187,   266,   187, 34502,   187, 28936,   187,\n",
      "           395,     3,   187,    66,   187, 29777, 39901,   187, 28936,   187,\n",
      "           266,   187, 34502,   187, 28936,   187,  3529,   187,  5658,   187,\n",
      "          5092,    13,   187, 12550,   187, 29777,   545, 20283,    13,   187,\n",
      "           262,   187,  5092,   187,  3088,   187,   635,   187, 25914,   187,\n",
      "          3062,   187,  2858,    13,   187,   262,   187, 22732,   187,  6309,\n",
      "           347,   187, 17124,   187,   395,   187, 29266,   187,   249,  2426,\n",
      "           187,  1171,   187,    85, 25004,   187, 12550,   187, 29777, 39901,\n",
      "           187,   395,   187,   262,   187, 22732,   187,  3062,   187,  2920,\n",
      "           187,   395,   187,    84, 46711,   187, 49831,   187, 12550,   187,\n",
      "         29777,  6198,   187,   395,   187,   262,   187,  9846,   187, 11145,\n",
      "           187,  3529,   187, 25914,   187, 29266]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1477d74-0633-46bb-bcd2-98ed8511a07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n}\\n\\nI want to create a list and list of the results of search. A list of the results are the values that I would like to have in the search result list. As with a string list, I am getting an error. I want to get a response, instead of a list of the results.\\n//A list of the results in the textfield\\n    List<EmployeeType> results;\\n    //Using the \"results\" parameter as a key for the result list to be returned\\n    IDictionary<Key<EmployeeType>, TResultList> resultsListKey = \\n                                '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b12d3f-cb37-4d21-a159-9de6ee36efc3",
   "metadata": {},
   "source": [
    "## Classifier-free guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6079f329-2405-48ee-abd8-8ee9201fa0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (GPT2Tokenizer, AutoModelForCausalLM,\n",
    "                          GPTNeoXForCausalLM, AutoTokenizer)\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (LogitsProcessor, LogitsProcessorList,\n",
    "                          MinLengthLogitsProcessor, TemperatureLogitsWarper,\n",
    "                          TopKLogitsWarper, TopPLogitsWarper,\n",
    "                          TypicalLogitsWarper)\n",
    "from transformers.generation import LogitNormalization\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CFGLogits(LogitsProcessor):\n",
    "    r\"\"\"Logits processor for Classifier-Free Guidance (CFG). The processors\n",
    "    computes a weighted average across scores from prompt conditional and prompt unconditional (or negative) logits,\n",
    "    parameterized by the `guidance_scale`. The unconditional scores are computed internally by prompting `model` with\n",
    "    the `uncond` branch. Finally, according to CFG Rescale, the reweighted logits are interpolated back with weight\n",
    "    `rescale_factor` the conditional ones to smooth the effect and increase output quality.\n",
    "\n",
    "    See [the paper](https://arxiv.org/abs/2306.17806) for more information.\n",
    "\n",
    "    Args:\n",
    "        guidance_scale (float):\n",
    "            The guidance scale for classifier free guidance (CFG). CFG is enabled by setting `guidance_scale > 1`.\n",
    "            Higher guidance scale encourages the model to generate samples that are more closely linked to the input\n",
    "            prompt, usually at the expense of poorer quality.\n",
    "        uncond (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary for the unconditional branch.\n",
    "        model:\n",
    "            The LM computing the unconditional scores. Supposedly the same as the one computing the conditional scores.\n",
    "            Both models must use the same tokenizer.\n",
    "        smooth_factor (float):\n",
    "            The interpolation weight for CFG Rescale. 1 means no rescaling, 0 reduces to the conditional scores without\n",
    "            CFG. Turn it lower if the output degenerates. Lower values allow for higher guidance scale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, guidance_scale, uncond, model, rescale_factor=1.0):\n",
    "        self.guidance_scale = guidance_scale\n",
    "        self.uncond = uncond\n",
    "        self.model = model\n",
    "        self.out = None\n",
    "        self.rescale_factor = rescale_factor\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        scores = F.log_softmax(scores, dim=-1)\n",
    "        if self.guidance_scale == 1:\n",
    "            return scores\n",
    "\n",
    "        if self.out is None:\n",
    "            self.out = self.model(self.uncond, use_cache=True)\n",
    "        else:\n",
    "            self.out = self.model(\n",
    "                input_ids[:, -1:],\n",
    "                use_cache=True,\n",
    "                past_key_values=self.out.past_key_values,\n",
    "            )\n",
    "        unconditional_logits = F.log_softmax(self.out.logits[0][-1:], dim=-1)\n",
    "        out = self.guidance_scale * (scores - unconditional_logits) + unconditional_logits\n",
    "        out = F.log_softmax(out, dim=-1)\n",
    "        if self.rescale_factor == 1:\n",
    "            return out\n",
    "        return self.rescale_factor * out + (1 - self.rescale_factor) * scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2040e7f-5947-4b52-b2fe-bbee1d3b997b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abfa3bf94954922a50c3c6a8b646df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58d1e7d631f4f64873036dfbcb17e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef911136fe534ffe917caf6b814d7645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac45856e3a3547b984a36d2ae42e5798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157dc386711f4c35b62a75ddd13ca582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# paper usage: (copying and editing @grantCelley 's answer)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import LogitsProcessorList, TemperatureLogitsWarper, TopPLogitsWarper\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e177080a-b3df-4146-9d00-53a53c29f2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today a dragon flew over Paris, France, and entered Paris, France.\n",
      "\n",
      "The dragon was the second most successful dragon of the 19th century and was one of the best known in France. It was a dragon of the early 19th century, but it took off in the early 20th century. The Dragon was also a great success on the world stage, becoming the world's biggest and most successful dragon ever, a big success in France and Germany, and one of the world's biggest dragon.\n",
      "\n",
      "The dragon has been around for over 300 years, and in the last 100 years, it is a great dragon. It is also the most difficult dragon\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer(\"Today a dragon flew over Paris, France,\", return_tensors='pt')\n",
    "# either provide a negative prompt:\n",
    "neg_prompt = tokenizer(\"A sad event happened,\", return_tensors='pt')['input_ids']\n",
    "# or don't:\n",
    "# neg_prompt = prompt['input_ids'][:, -1:]\n",
    "\n",
    "device='cuda:0'\n",
    "model.to(device)\n",
    "outputs = model.generate(\n",
    "    input_ids=prompt['input_ids'].to(device),\n",
    "    attention_mask=prompt['attention_mask'].to(device),\n",
    "    max_new_tokens=125,\n",
    "    logits_processor=LogitsProcessorList([\n",
    "        # inputs_cfg usually is the last token of the prompt but there are\n",
    "        # possibilities of negative prompting that are explored in the paper\n",
    "        CFGLogits(1.5, neg_prompt.to(device), model),\n",
    "        TemperatureLogitsWarper(0.8),\n",
    "        TopPLogitsWarper(0.95),\n",
    "    ]),\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a5919-967f-4be1-a489-27b7dc65638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(model, tokenizer, prompt, neg_prompts, num_examples, device='cuda:0'):\n",
    "    model.to(device)\n",
    "    generated_examples = []\n",
    "\n",
    "    for _ in range(num_examples):\n",
    "        # Generate a new example\n",
    "        prompt_tensor = tokenizer(prompt, return_tensors='pt')\n",
    "        neg_prompts_tensor = [tokenizer(neg_prompt, return_tensors='pt')['input_ids'] for neg_prompt in neg_prompts]\n",
    "        \n",
    "        # Ensure everything is on the right device\n",
    "        prompt_tensor = {k: v.to(device) for k, v in prompt_tensor.items()}\n",
    "        neg_prompts_tensor = [prompt.to(device) for prompt in neg_prompts_tensor]\n",
    "\n",
    "        output = model.generate(\n",
    "            input_ids=prompt_tensor['input_ids'],\n",
    "            attention_mask=prompt_tensor['attention_mask'],\n",
    "            max_new_tokens=125,\n",
    "            logits_processor=LogitsProcessorList([\n",
    "                CFGLogits(1.5, neg_prompt, model) for neg_prompt in neg_prompts_tensor,\n",
    "                TemperatureLogitsWarper(0.8),\n",
    "                TopPLogitsWarper(0.95),\n",
    "            ]),\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "        generated_example = tokenizer.decode(output[0])\n",
    "        generated_examples.append(generated_example)\n",
    "\n",
    "        # Add generated example to the set of negative prompts\n",
    "        neg_prompts.append(generated_example)\n",
    "\n",
    "    return generated_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff85157-a874-425f-8813-881e1b6c2903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
